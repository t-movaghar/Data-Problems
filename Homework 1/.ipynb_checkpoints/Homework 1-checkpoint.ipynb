{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef861bbd",
   "metadata": {},
   "source": [
    "# Unix Shell\n",
    "\n",
    "There is a lot that can be done on the Unix shell command prompt. For homework, we will do some useful manipulations of CSV files.\n",
    "\n",
    "There is plenty of material online that will help you figure out how to do various tasks on the command line. Some example resources I found by googling:\n",
    "\n",
    "* Paths and Wildcards: https://www.warp.dev/terminus/linux-wildcards\n",
    "* General introduction to shell: https://github-pages.ucl.ac.uk/RCPSTrainingMaterials/HPCandHTCusingLegion/2_intro_to_shell.html\n",
    "* Manual pages: https://www.geeksforgeeks.org/linux-man-page-entries-different-types/?ref=ml_lbp\n",
    "* Chaining commands: https://www.geeksforgeeks.org/chaining-commands-in-linux/?ref=ml_lbp\n",
    "* Piping: https://www.geeksforgeeks.org/piping-in-unix-or-linux/\n",
    "* Using sed: https://www.geeksforgeeks.org/sed-command-linux-set-2/?ref=ml_lbp\n",
    "* Various Unix commands: https://www.geeksforgeeks.org/linux-commands/?ref=lbp\n",
    "* Cheat sheets:\n",
    "    * https://www.stationx.net/unix-commands-cheat-sheet/\n",
    "    * https://cheatography.com/davechild/cheat-sheets/linux-command-line/\n",
    "    * https://www.theknowledgeacademy.com/blog/unix-commands-cheat-sheet/\n",
    "    \n",
    "These aren't necessarily the best resource. Feel free to search for better ones. Also, don't forget that Unix has built-in manual pages for all of its commands. Just type `man <command>` at the command prompt. Use the space-bar to scroll through the documentation and \"q\" to exit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ba2683",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "Perform all of these tasks on the Unix command prompt. Some may require several commands. Many will require chaining commands together. Once you figure out how to perform the task, copy paste the command(s) here.  \n",
    "\n",
    "1. After unziping the Kaggle CSV files, make a new directory for the original zip files, and move the files there. In case you accidentally mess up one of the CSV files, you'll be able unzip the data again. \n",
    "\n",
    "Hint: use `mkdir` and `mv` commands with appropriate wildcards.\n",
    "\n",
    "2. The \"diabetes_prediction_dataset.csv\" file has a lot of entries. Create 3 new CSV files, each with about 1/3 of the data.\n",
    "\n",
    "Hints: \n",
    "* Use `head` to get first line.  \n",
    "* First create 3 files with just the first line by redirecting output of `head` into a file using `>`.\n",
    "* Use `wc` to count the number of entries\n",
    "* Chain/pipe `head` and `tail` to select specific lines, redirecting output to append to the 3 files you created using `>>`.\n",
    "\n",
    "3. Create 2 new CSV files from `Heart_Disease_Prediction.csv`, one containing rows with \"Presence\" label and another with \"Absence\" label. Make sure that the first line of each file contains the field names. \n",
    "\n",
    "Hints: \n",
    "* Use `head` to get first line.  \n",
    "* First create 2 files with just the first line by redirecting output of `head` into a file using `>`.\n",
    "* Use `grep` to select lines that contain \"Absence\" or \"Presence\" and append the output to the appropriate file created in the previous step.\n",
    "\n",
    "4. What fraction of cars in `car_web_scraped_dataset.csv` have had no accidents?\n",
    "\n",
    "Hints:\n",
    "* Use `grep` to select the appropriate lines.\n",
    "* Pipe the output of grep into `wc` (using `|`) to count the lines.\n",
    "\n",
    "5. Make the following replacements in `Housing.csv`, output the result into a new CSV:\n",
    "\n",
    "* yes -> 1\n",
    "* no -> 0\n",
    "* unfurnished -> 0\n",
    "* furnished -> 1\n",
    "* semi-furnished -> 2\n",
    "    \n",
    "Hints:\n",
    "* Use `sed` to do the replacement.\n",
    "* Use pipes to chain multiple `sed` commands.\n",
    "* To avoid replacing \"unfurnished\" or \"semi-furnished\" when performing the \"furnished\" replacement, try replacing \",furnished\" with \",1\".\n",
    "\n",
    "6. Create a new CSV files from `Mall_Customers`, removing \"CustomerID\" column.\n",
    "\n",
    "Hints:\n",
    "* Use `cut` command\n",
    "* Default separator for `cut` is the space character. For CSV, you have to use option `-d ','`.\n",
    "\n",
    "7. Create a new file that contains the sum of the following fields for each row:\n",
    "    * Research Quality Score\n",
    "    * Industry Score\n",
    "    * International Outlook\n",
    "    * Research Environment Score\n",
    "    \n",
    "Hints:\n",
    "* Use `cut` to select the correct columns.\n",
    "* Use `tr` to replace ',' with '+'.\n",
    "* Pipe output into `bc` to compute the sum.\n",
    "\n",
    "8. Sort the \"cancer patient data sets.csv\" file by age. Make sure the output is a readable CSV file.\n",
    "\n",
    "Hints:\n",
    "* Use sort with `-n`, `-t`, and `-k` options. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e35768-6c9e-434a-91db-72400f1b1a69",
   "metadata": {},
   "source": [
    "## Question 1:\n",
    "* unzip '*.zip'\n",
    "* mkdir 'Unzipped Files'\n",
    "* mv *.zip 'Unzipped Files'\n",
    "\n",
    "## Question 2:\n",
    "* head -n 1 diabetes_prediction_dataset.csv > diabetes_1.csv\n",
    "* head -n 1 diabetes_prediction_dataset.csv > diabetes_2.csv\n",
    "* head -n 1 diabetes_prediction_dataset.csv > diabetes_3.csv\n",
    "* wc -l diabetes_prediction_dataset.csv\n",
    "* total_lines=100001\n",
    "* `N=$((total_lines / 3))`\n",
    "* `head -n 1 diabetes_prediction_dataset.csv > diabetes_1.csv`\n",
    "*  `tail -n +2 diabetes_prediction_dataset.csv | head -n $N >> diabetes_1.csv`\n",
    " * `head -n 1 diabetes_prediction_dataset.csv > diabetes_2.csv`\n",
    "*  `tail -n +$((N + 2)) diabetes_prediction_dataset.csv | head -n $N >> diabetes_2.csv`\n",
    " * `head -n 1 diabetes_prediction_dataset.csv > diabetes_3.csv`\n",
    " * `tail -n +$((2 * N + 2)) diabetes_prediction_dataset.csv | head -n $N >> diabetes_3.csv`.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da9e8c-3a3b-4c85-9f2e-2818574f18ff",
   "metadata": {},
   "source": [
    "\n",
    "## Question 3:\n",
    "* `head -n 1 Heart_Disease_Prediction.csv > Presence_Label.csv`\n",
    "* `grep \"Presence\" Heart_Disease_Prediction.csv >> Presence_Label.csv`\n",
    "\n",
    "* `head -n 1 Heart_Disease_Prediction.csv > Absence_Label.csv`\n",
    "* `grep \"Absence\" Heart_Disease_Prediction.csv >> Absence_Label.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d843134f-4634-474f-96d9-0d7964b0de91",
   "metadata": {},
   "source": [
    "## Question 4:\n",
    "* `grep -c \"No accident\" car_web_scraped_dataset.csv`\n",
    "* Output is 2223\n",
    "* `wc -l car_web_scraped_dataset.csv`\n",
    "* Output is 2841\n",
    "* Excluding header line, fraction is 2223/2840"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e5fe5b-a3d2-40b5-bdd7-fd7c97627a39",
   "metadata": {},
   "source": [
    "## Question 5:\n",
    "* ` sed 's/yes/1/g; s/no/0/g; s/unfurnished/0/g; s/,furnished/,1/g; s/semi-furnished/2/g' Housing.csv > Encoded_Housing.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba0737a-5a8f-47a2-8c98-df078e54bac6",
   "metadata": {},
   "source": [
    "## Question 6:\n",
    "* `cut -d ',' -f 2- Mall_Customers.csv > NO_ID_Mall_Customers.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b497247f-3cc6-441e-91ec-ad9ba41c90ff",
   "metadata": {},
   "source": [
    "## Question 7:\n",
    "* `cut -d ',' -f 5,6,7,8 \"world all university rank and rank score.csv\" | tr ',' '+' | bc > Sum_Scores.csv `\n",
    "* I keep getting a lengthy error, and I've lost a few lines of data. I can't find any non-number characters in the specified columns...\n",
    "* `(standard_in) 1: syntax error\r\n",
    "(standard_in) 1: syntax error\r\n",
    "(standard_in) 1: syntax error\r\n",
    "(standard_in) 1: syntax error\r\n",
    "(standard_in) 162: illegal character: \\342\r\n",
    "(standard_in) 162: illegal character: \\200\r\n",
    "(standard_in) 162: illegal character`: \\223: \\223"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657ce47-43d8-4216-9e1c-25790e49c8c9",
   "metadata": {},
   "source": [
    "## Question 8:\n",
    "\n",
    "* `sort -n -t ',' -k 3 \"cancer patient data sets.csv\" > age_sorted_cancer_patient_data.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2af8a6-48ef-466d-bfb3-eb1e479824f8",
   "metadata": {},
   "source": [
    "## Final ls\n",
    "\n",
    "* ` Absence_Label.csv              NO_ID_Mall_Customers.csv   age_sorted_cancer_patient_data.csv      diabetes_2.csv                    starbucks.csv\r\n",
    " Encoded_Housing.csv            Presence_Label.csv         breast-cancer-wisconsin-data_data.csv   diabetes_3.csv                   'world all university rank and rank score.csv'\r\n",
    " Heart_Disease_Prediction.csv   Sum_Scores.csv            'cancer patient data sets.csv'           diabetes_prediction_dataset.csv\r\n",
    " Housing.csv                    Titanic-Dataset.csv        car_web_scraped_dataset.csv             housing_price_dataset.csv\r\n",
    " Mall_Customers.csv            'Unzipped Files'            diabetes_1.cs    seattle-weather`.csv1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d676ed2d-838c-4591-8028-3def28d34e24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
